{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Importing Libraries**"
      ],
      "metadata": {
        "id": "fxHyZODD4jCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install gdown and OpenCV\n",
        "!pip install gdown\n",
        "!pip install opencv-python\n",
        "\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import pathlib\n",
        "import gdown\n",
        "import numpy as np\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "yc2g0RQIOUjq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80bc7ad0-649d-4aff-bcf4-159d157c3d82"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.1.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.15.4)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.7.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.25.2)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Download and Extract Dataset**\n",
        "\n",
        "This section handles downloading the dataset from Google Drive and extracting it to the specified directory."
      ],
      "metadata": {
        "id": "DmEJuUVt4cQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "base_dir = '/content/drive/MyDrive/dk-dataset'\n",
        "if not os.path.exists(base_dir):\n",
        "    os.makedirs(base_dir)\n",
        "\n",
        "zip_file_path = os.path.join(base_dir, 'dk-dataset.zip')\n",
        "\n",
        "# Google Drive file ID\n",
        "file_id = '1JWpW6JTdV__L-j18QU3wgGrcIdguHl8l'\n",
        "\n",
        "# Construct the download URL\n",
        "download_url = f'https://drive.google.com/uc?id={file_id}'\n",
        "\n",
        "# Download the dataset\n",
        "gdown.download(download_url, zip_file_path, quiet=False)\n",
        "\n",
        "# Unzip the dataset\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zf:\n",
        "    zf.extractall(base_dir)\n",
        "\n",
        "dataset_path = os.path.join(base_dir, 'dataset')\n",
        "\n",
        "data_dir = pathlib.Path(dataset_path)\n",
        "\n",
        "if data_dir.exists() and data_dir.is_dir():\n",
        "    print(\"Data directory exists and is a directory.\")\n",
        "    print(\"Subdirectories in the dataset:\")\n",
        "    for sub_dir in os.listdir(data_dir):\n",
        "        full_path = os.path.join(data_dir, sub_dir)\n",
        "        if os.path.isdir(full_path) and not sub_dir.startswith('.'):\n",
        "            print(f'  - {sub_dir}{os.path.sep}')\n",
        "else:\n",
        "    print(f\"Error: The directory {data_dir} does not exist or is not a directory.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsHRgn3JOVNc",
        "outputId": "93797108-413f-4676-d7c2-58e4c5e0888e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1JWpW6JTdV__L-j18QU3wgGrcIdguHl8l\n",
            "From (redirected): https://drive.google.com/uc?id=1JWpW6JTdV__L-j18QU3wgGrcIdguHl8l&confirm=t&uuid=bf5eda76-b11c-4c52-8ca0-69e9ae1b0c53\n",
            "To: /content/drive/MyDrive/dk-dataset/dk-dataset.zip\n",
            "100%|██████████| 210M/210M [00:12<00:00, 16.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data directory exists and is a directory.\n",
            "Subdirectories in the dataset:\n",
            "  - train/\n",
            "  - test/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define directories for training and testing\n",
        "\n",
        "train_pos_dir = os.path.join(data_dir, 'train/positive')\n",
        "train_neg_dir = os.path.join(data_dir, 'train/negative')\n",
        "test_dir = os.path.join(data_dir, 'test')\n"
      ],
      "metadata": {
        "id": "qIVwprGHjE-X"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Data Preprocessing**\n",
        "\n",
        "This section preprocesses the images for the watermark detection model. It includes loading the images, resizing them, normalizing pixel values, and splitting the data into training, validation, and test sets."
      ],
      "metadata": {
        "id": "479HvRk54MH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import pathlib\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "\n",
        "dataset_path = '/content/drive/MyDrive/dk-dataset/dataset'\n",
        "data_dir = pathlib.Path(dataset_path)\n",
        "\n",
        "# Define directories for training and testing\n",
        "train_pos_dir = os.path.join(data_dir, 'train/positive')\n",
        "train_neg_dir = os.path.join(data_dir, 'train/negative')\n",
        "test_dir = os.path.join(data_dir, 'test')\n",
        "\n",
        "\n",
        "def load_and_preprocess_image(image_path, img_size=(224, 224)):\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        raise ValueError(f\"Unable to load image at path: {image_path}\")\n",
        "    image = cv2.resize(image, img_size)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image = image / 255.0  # Normalize pixel values\n",
        "    return image\n",
        "\n",
        "\n",
        "def load_training_data():\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    # Positive images\n",
        "    for filename in os.listdir(train_pos_dir):\n",
        "        image_path = os.path.join(train_pos_dir, filename)\n",
        "        try:\n",
        "            image = load_and_preprocess_image(image_path)\n",
        "            images.append(image)\n",
        "            labels.append(1)  # Label for positive\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {image_path}: {e}\")\n",
        "\n",
        "\n",
        "    for filename in os.listdir(train_neg_dir):\n",
        "        image_path = os.path.join(train_neg_dir, filename)\n",
        "        try:\n",
        "            image = load_and_preprocess_image(image_path)\n",
        "            images.append(image)\n",
        "            labels.append(0)  # Label for negative\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {image_path}: {e}\")\n",
        "\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "\n",
        "def load_testing_data():\n",
        "    images = []\n",
        "    filenames = []\n",
        "\n",
        "    for filename in os.listdir(test_dir):\n",
        "        image_path = os.path.join(test_dir, filename)\n",
        "        try:\n",
        "            image = load_and_preprocess_image(image_path)\n",
        "            images.append(image)\n",
        "            filenames.append(filename)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {image_path}: {e}\")\n",
        "\n",
        "    return np.array(images), filenames\n",
        "\n",
        "\n",
        "X_train, y_train = load_training_data()\n",
        "X_test, test_filenames = load_testing_data()\n",
        "\n",
        "\n",
        "y_train = to_categorical(y_train, num_classes=2)\n",
        "\n",
        "# Split training data into train and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training data shape: {X_train.shape}\")\n",
        "print(f\"Validation data shape: {X_val.shape}\")\n",
        "print(f\"Testing data shape: {X_test.shape}\")\n"
      ],
      "metadata": {
        "id": "F5-jHOhaglVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Define and Compile the VGG16 Model**\n",
        "\n",
        "This section sets up the VGG16 model for the watermark detection task, including adding custom layers and compiling the model."
      ],
      "metadata": {
        "id": "Db73pOcJ4FeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "\n",
        "x = base_model.output\n",
        "x = Flatten()(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "predictions = Dense(2, activation='softmax')(x)\n",
        "\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(model.summary())\n"
      ],
      "metadata": {
        "id": "OvpbUhZOiT3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Train the Model**\n",
        "\n",
        "This section trains the model using the prepared training data and evaluates its performance on the validation set. It also plots training and validation accuracy and loss over the epochs."
      ],
      "metadata": {
        "id": "CZWEiEKG4DHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from data_preprocessing import X_train, X_val, y_train, y_val\n",
        "from model import model\n",
        "\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
        "\n",
        "\n",
        "val_loss, val_accuracy = model.evaluate(X_val, y_val)\n",
        "print(f'Validation Accuracy: {val_accuracy:.4f}')\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FqEozaCjhuNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Display Sample Images**\n",
        "\n",
        "This section visualizes sample images from the training dataset to give an overview of the types of images used for training."
      ],
      "metadata": {
        "id": "jzKvP_BX38Bq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "train_pos_dir = '/content/drive/MyDrive/dk-dataset/dataset/train/positive'\n",
        "train_neg_dir = '/content/drive/MyDrive/dk-dataset/dataset/train/negative'\n",
        "\n",
        "\n",
        "def display_sample_images(dir_path, num_samples=10, has_watermark=True):\n",
        "    files = os.listdir(dir_path)\n",
        "    count = 0\n",
        "    plt.figure(figsize=(12, 12))\n",
        "\n",
        "    for filename in files:\n",
        "        if count >= num_samples:\n",
        "            break\n",
        "\n",
        "        image_path = os.path.join(dir_path, filename)\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        plt.subplot(1, num_samples, count + 1)\n",
        "        plt.imshow(image)\n",
        "        plt.axis('off')\n",
        "        title = 'Watermark' if has_watermark else 'No Watermark'\n",
        "        plt.title(title)\n",
        "        count += 1\n",
        "\n",
        "# Display some positive images (with watermark)\n",
        "print(\"Sample images with watermark:\")\n",
        "display_sample_images(train_pos_dir, num_samples=10, has_watermark=True)\n",
        "\n",
        "# Display some negative images (without watermark)\n",
        "print(\"\\nSample images without watermark:\")\n",
        "display_sample_images(train_neg_dir, num_samples=10, has_watermark=False)\n"
      ],
      "metadata": {
        "id": "qKvO0XiHhVYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Save the Trained Model**\n",
        "\n",
        "This section saves the trained model to a file for future use."
      ],
      "metadata": {
        "id": "LqnPgj273zda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from model import model\n",
        "\n",
        "\n",
        "model_save_path = '/content/drive/MyDrive/dk-dataset/vgg16_watermark_detection_model.h5'\n",
        "\n",
        "\n",
        "model.save(model_save_path)\n",
        "print(f\"Model saved to {model_save_path}\")\n"
      ],
      "metadata": {
        "id": "nAeW7-LpiD4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Load the Model and Make Predictions**\n",
        "\n",
        "This section demonstrates how to load the saved model and use it to make predictions on new test images."
      ],
      "metadata": {
        "id": "9bSrtdqV3tJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "\n",
        "model_load_path = '/content/drive/MyDrive/dk-dataset/vgg16_watermark_detection_model.h5'\n",
        "\n",
        "model = load_model(model_load_path)\n",
        "\n",
        "\n",
        "test_dir = '/content/drive/MyDrive/dk-dataset/dataset/test'\n",
        "\n",
        "\n",
        "def load_and_preprocess_image(image_path, img_size=(224, 224)):\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        raise ValueError(f\"Unable to load image at path: {image_path}\")\n",
        "    image = cv2.resize(image, img_size)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image = image / 255.0\n",
        "    return image\n",
        "\n",
        "\n",
        "def predict_images(test_dir):\n",
        "    filenames = []\n",
        "    predictions = []\n",
        "    for filename in os.listdir(test_dir):\n",
        "        image_path = os.path.join(test_dir, filename)\n",
        "        try:\n",
        "            image = load_and_preprocess_image(image_path)\n",
        "            image = np.expand_dims(image, axis=0)\n",
        "            pred = model.predict(image)\n",
        "            class_index = np.argmax(pred, axis=1)[0]\n",
        "            class_label = 'Watermark' if class_index == 1 else 'No Watermark'\n",
        "            filenames.append(filename)\n",
        "            predictions.append(class_label)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {image_path}: {e}\")\n",
        "\n",
        "    return filenames, predictions\n",
        "\n",
        "\n",
        "filenames, predictions = predict_images(test_dir)\n",
        "\n",
        "# Print predictions\n",
        "for filename, prediction in zip(filenames, predictions):\n",
        "    print(f'{filename}: {prediction}')\n"
      ],
      "metadata": {
        "id": "_-1jhnHq2Dj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y98X15s92Fcv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}